{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install \"numpy<2\"\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND TESTING\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "  def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "    \n",
    "    self.data_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    if train:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "    else:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "    \n",
    "    self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data_frame.iloc[idx]\n",
    "    filename = row['filename']\n",
    "    \n",
    "    img_path = os.path.join(self.root_dir, filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "  \n",
    "    lower_filename = filename.lower()\n",
    "    if \"benign\" in lower_filename:\n",
    "      label = 0\n",
    "    elif \"malignant\" in lower_filename:\n",
    "      label = 1\n",
    "    else:\n",
    "      raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1, 2)\n",
    "    return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "    x = torch.cat((tokens_batch, x), dim=1)\n",
    "    x = x + self.pe\n",
    "    return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "    attention = Q @ K.transpose(-2, -1)\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "    out = attention @ V\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.W_o(out)\n",
    "    return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.dropout1 = nn.Dropout(dropout_prob)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        nn.Linear(d_model * r_mlp, d_model),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    attn_out = self.mha(self.ln1(x))\n",
    "    x = x + self.dropout1(attn_out)\n",
    "    mlp_out = self.mlp(self.ln2(x))\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "        \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_classes = n_classes\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(\n",
    "        *[TransformerEncoder(self.d_model, self.n_heads, r_mlp=4, dropout_prob=dropout_prob)\n",
    "          for _ in range(n_layers)]\n",
    "    )\n",
    "    self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[:, 0])\n",
    "    return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 9\n",
    "n_classes = 2 # Binary classification: 0 for benign, 1 for malignant\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 3\n",
    "n_layers = 8\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "alpha = 0.001\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "train_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=True, transform=train_transform)\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(\"Training set grp values:\", train_set.data_frame['grp'].unique())\n",
    "print(\"Testing set grp values:\", test_set.data_frame['grp'].unique())\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "  print(\"Using device:\", device)\n",
    "\n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "optimizer = AdamW(transformer.parameters(), lr=alpha)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "  transformer.train()\n",
    "  training_loss = 0.0\n",
    "  for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    labels = labels.float().unsqueeze(1)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    training_loss += loss.item()\n",
    "  end = time.time()\n",
    "  curr_lr = scheduler.get_last_lr()\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss / len(train_loader):.3f} time: {end-start:.2f} sec lr: {curr_lr}')\n",
    "  if ((epoch+1) % 10) == 0:\n",
    "    torch.save(transformer.state_dict(), f\"./checkpoints/checkpoint:{epoch+1}-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}.pth\")\n",
    "\n",
    "# Testing\n",
    "def test_model(model, test_loader, device):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      predicted = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "      labels = labels.float().unsqueeze(1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy\n",
    "\n",
    "test_accuracy = test_model(transformer, test_loader, device)\n",
    "\n",
    "# Save the final model\n",
    "model_scripted = torch.jit.script(transformer)\n",
    "model_scripted.save(f\"./models/binary/{test_accuracy:.2f}%-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
