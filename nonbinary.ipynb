{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install \"numpy<2\"\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND TESTING\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "  def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "    \n",
    "    self.data_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    if train:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "    else:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "    \n",
    "    self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data_frame.iloc[idx]\n",
    "    filename = row['filename']\n",
    "    \n",
    "    img_path = os.path.join(self.root_dir, filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "  \n",
    "    lower_filename = filename.lower()\n",
    "    if \"adenosis\" in lower_filename:\n",
    "      label = 0\n",
    "    elif \"fibroadenoma\" in lower_filename:\n",
    "      label = 1\n",
    "    elif \"phyllodes_tumor\" in lower_filename:\n",
    "      label = 2\n",
    "    elif \"tubular_adenoma\" in lower_filename:\n",
    "      label = 3\n",
    "    elif \"ductal_carcinoma\" in lower_filename:\n",
    "      label = 4\n",
    "    elif \"lobular_carcinoma\" in lower_filename:\n",
    "      label = 5\n",
    "    elif \"mucinous_carcinoma\" in lower_filename:\n",
    "      label = 6\n",
    "    elif \"papillary_carcinoma\" in lower_filename:\n",
    "      label = 7\n",
    "    else:\n",
    "      raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1, 2)\n",
    "    return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "    x = torch.cat((tokens_batch, x), dim=1)\n",
    "    x = x + self.pe\n",
    "    return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "    attention = Q @ K.transpose(-2, -1)\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "    out = attention @ V\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.W_o(out)\n",
    "    return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model * r_mlp),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(d_model * r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "    return out\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "    super().__init__()\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "      \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_classes = n_classes\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(\n",
    "      *[TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)]\n",
    "    )\n",
    "    self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[:, 0])\n",
    "    return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "\n",
    "transform = T.Compose([\n",
    "  T.Resize(img_size),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "  T.Resize(img_size),\n",
    "  T.RandomRotation(degrees=180),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "train_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=True, transform=test_transform)\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "  print(\"Using device:\", device)\n",
    "\n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "optimizer = AdamW(transformer.parameters(), lr=alpha)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "  transformer.train()\n",
    "  training_loss = 0.0\n",
    "  for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    training_loss += loss.item()\n",
    "  end = time.time()\n",
    "  curr_lr = scheduler.get_last_lr()\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss / len(train_loader):.3f} time: {end-start:.2f} sec lr: {curr_lr}')\n",
    "  if ((epoch+1) % 10) == 0:\n",
    "    model_scripted = torch.jit.script(transformer)\n",
    "    model_scripted.save(f\"./checkpoints/checkpoint:{epoch+1}-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}.pth\")\n",
    "\n",
    "# Testing\n",
    "\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy\n",
    "\n",
    "test_accuracy = test_model(transformer, test_loader, device)\n",
    "\n",
    "# Save the final model\n",
    "model_scripted = torch.jit.script(transformer)\n",
    "model_scripted.save(f\"./models/nonbinary/{test_accuracy:.2f}%-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING ONLY\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "  def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "    \n",
    "    self.data_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    if train:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "    else:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "    \n",
    "    self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data_frame.iloc[idx]\n",
    "    filename = row['filename']\n",
    "    \n",
    "    img_path = os.path.join(self.root_dir, filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "  \n",
    "    lower_filename = filename.lower()\n",
    "    if \"adenosis\" in lower_filename:\n",
    "      label = 0\n",
    "    elif \"fibroadenoma\" in lower_filename:\n",
    "      label = 1\n",
    "    elif \"phyllodes_tumor\" in lower_filename:\n",
    "      label = 2\n",
    "    elif \"tubular_adenoma\" in lower_filename:\n",
    "      label = 3\n",
    "    elif \"ductal_carcinoma\" in lower_filename:\n",
    "      label = 4\n",
    "    elif \"lobular_carcinoma\" in lower_filename:\n",
    "      label = 5\n",
    "    elif \"mucinous_carcinoma\" in lower_filename:\n",
    "      label = 6\n",
    "    elif \"papillary_carcinoma\" in lower_filename:\n",
    "      label = 7\n",
    "    else:\n",
    "      raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1, 2)\n",
    "    return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "    x = torch.cat((tokens_batch, x), dim=1)\n",
    "    x = x + self.pe\n",
    "    return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "    attention = Q @ K.transpose(-2, -1)\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "    out = attention @ V\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.W_o(out)\n",
    "    return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model * r_mlp),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(d_model * r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "    return out\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "    super().__init__()\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "      \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_classes = n_classes\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(\n",
    "      *[TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)]\n",
    "    )\n",
    "    self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[:, 0])\n",
    "    return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "\n",
    "transform = T.Compose([\n",
    "  T.Resize(img_size),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "  T.Resize(img_size),\n",
    "  T.RandomRotation(degrees=180),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "# CHANGE WHEN LOADING A MODEL\n",
    "model = torch.jit.load('./models/nonbinary/98.19%-16-8-(256, 256)-(16, 16)-3-4-8-256-200-0.001.pth')\n",
    "\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "  print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy\n",
    "\n",
    "test_accuracy = test_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
