{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (NVIDIA GeForce RTX 4050 Laptop GPU)\n",
      "Epoch 1/200 loss: 1.666 time: 71.26 sec lr: [0.0004065803829630208]\n",
      "Epoch 2/200 loss: 1.531 time: 72.83 sec lr: [0.00042630348958544624]\n",
      "Epoch 3/200 loss: 1.519 time: 70.41 sec lr: [0.0004591152425361105]\n",
      "Epoch 4/200 loss: 1.462 time: 66.95 sec lr: [0.0005049256776899699]\n",
      "Epoch 5/200 loss: 1.449 time: 63.82 sec lr: [0.0005636091907941006]\n",
      "Epoch 6/200 loss: 1.421 time: 64.03 sec lr: [0.0006350048818527364]\n",
      "Epoch 7/200 loss: 1.391 time: 63.14 sec lr: [0.0007189169962870962]\n",
      "Epoch 8/200 loss: 1.319 time: 63.77 sec lr: [0.0008151154616604216]\n",
      "Epoch 9/200 loss: 1.259 time: 63.63 sec lr: [0.0009233365184966279]\n",
      "Epoch 10/200 loss: 1.157 time: 63.98 sec lr: [0.001043283443462946]\n",
      "Epoch 11/200 loss: 1.071 time: 63.52 sec lr: [0.0011746273629337396]\n",
      "Epoch 12/200 loss: 0.987 time: 61.43 sec lr: [0.0013170081547048546]\n",
      "Epoch 13/200 loss: 0.901 time: 63.85 sec lr: [0.0014700354353861089]\n",
      "Epoch 14/200 loss: 0.846 time: 62.16 sec lr: [0.0016332896307647247]\n",
      "Epoch 15/200 loss: 0.764 time: 62.52 sec lr: [0.001806323126204901]\n",
      "Epoch 16/200 loss: 0.730 time: 63.90 sec lr: [0.001988661493929381]\n",
      "Epoch 17/200 loss: 0.696 time: 62.82 sec lr: [0.0021798047938179615]\n",
      "Epoch 18/200 loss: 0.639 time: 63.29 sec lr: [0.0023792289441564446]\n",
      "Epoch 19/200 loss: 0.590 time: 64.10 sec lr: [0.002586387158577618]\n",
      "Epoch 20/200 loss: 0.579 time: 63.88 sec lr: [0.0028007114452544713]\n",
      "Epoch 21/200 loss: 0.586 time: 61.81 sec lr: [0.0030216141642350836]\n",
      "Epoch 22/200 loss: 0.541 time: 63.02 sec lr: [0.003248489638649262]\n",
      "Epoch 23/200 loss: 0.482 time: 62.27 sec lr: [0.0034807158153692718]\n",
      "Epoch 24/200 loss: 0.487 time: 62.39 sec lr: [0.003717655970571422]\n",
      "Epoch 25/200 loss: 0.491 time: 63.38 sec lr: [0.003958660455522151]\n",
      "Epoch 26/200 loss: 0.428 time: 64.17 sec lr: [0.004203068477801967]\n",
      "Epoch 27/200 loss: 0.438 time: 62.52 sec lr: [0.004450209913083438]\n",
      "Epoch 28/200 loss: 0.390 time: 61.09 sec lr: [0.004699407142495654]\n",
      "Epoch 29/200 loss: 0.395 time: 63.39 sec lr: [0.004949976910537425]\n",
      "Epoch 30/200 loss: 0.409 time: 63.00 sec lr: [0.005201232198445138]\n",
      "Epoch 31/200 loss: 0.411 time: 63.04 sec lr: [0.005452484107878828]\n",
      "Epoch 32/200 loss: 0.470 time: 62.27 sec lr: [0.00570304374976172]\n",
      "Epoch 33/200 loss: 0.353 time: 62.76 sec lr: [0.005952224133094366]\n",
      "Epoch 34/200 loss: 0.392 time: 63.52 sec lr: [0.0061993420485646]\n",
      "Epoch 35/200 loss: 0.354 time: 62.40 sec lr: [0.006443719941788748]\n",
      "Epoch 36/200 loss: 0.352 time: 61.65 sec lr: [0.006684687771048013]\n",
      "Epoch 37/200 loss: 0.421 time: 64.12 sec lr: [0.00692158484442644]\n",
      "Epoch 38/200 loss: 0.317 time: 65.45 sec lr: [0.0071537616313133425]\n",
      "Epoch 39/200 loss: 0.325 time: 64.12 sec lr: [0.007380581543303369]\n",
      "Epoch 40/200 loss: 0.531 time: 63.98 sec lr: [0.007601422679611298]\n",
      "Epoch 41/200 loss: 0.291 time: 64.72 sec lr: [0.007815679532215923]\n",
      "Epoch 42/200 loss: 0.295 time: 64.22 sec lr: [0.008022764646057829]\n",
      "Epoch 43/200 loss: 0.366 time: 65.01 sec lr: [0.008222110229739095]\n",
      "Epoch 44/200 loss: 0.295 time: 65.29 sec lr: [0.008413169712308632]\n",
      "Epoch 45/200 loss: 0.300 time: 64.82 sec lr: [0.00859541924186476]\n",
      "Epoch 46/200 loss: 0.362 time: 65.05 sec lr: [0.008768359121866106]\n",
      "Epoch 47/200 loss: 0.250 time: 65.24 sec lr: [0.008931515181212669]\n",
      "Epoch 48/200 loss: 0.283 time: 63.32 sec lr: [0.009084440074340604]\n",
      "Epoch 49/200 loss: 0.314 time: 65.10 sec lr: [0.009226714507766007]\n",
      "Epoch 50/200 loss: 0.302 time: 64.80 sec lr: [0.009357948389714803]\n",
      "Epoch 51/200 loss: 0.322 time: 64.73 sec lr: [0.009477781899686597]\n",
      "Epoch 52/200 loss: 0.278 time: 65.31 sec lr: [0.009585886475019964]\n",
      "Epoch 53/200 loss: 0.283 time: 64.60 sec lr: [0.009681965711754175]\n",
      "Epoch 54/200 loss: 0.406 time: 64.87 sec lr: [0.009765756177317356]\n",
      "Epoch 55/200 loss: 0.251 time: 65.67 sec lr: [0.009837028132812814]\n",
      "Epoch 56/200 loss: 0.422 time: 64.85 sec lr: [0.009895586162923182]\n",
      "Epoch 57/200 loss: 0.249 time: 64.81 sec lr: [0.009941269711705255]\n",
      "Epoch 58/200 loss: 0.256 time: 64.85 sec lr: [0.009973953522806495]\n",
      "Epoch 59/200 loss: 0.240 time: 66.49 sec lr: [0.00999354798289618]\n",
      "Epoch 60/200 loss: 0.291 time: 64.92 sec lr: [0.00999999987900105]\n",
      "Epoch 61/200 loss: 0.232 time: 68.46 sec lr: [0.009998716377065266]\n",
      "Epoch 62/200 loss: 0.221 time: 66.07 sec lr: [0.009994915880967953]\n",
      "Epoch 63/200 loss: 0.231 time: 62.36 sec lr: [0.00998860030437335]\n",
      "Epoch 64/200 loss: 0.197 time: 64.64 sec lr: [0.009979772827364463]\n",
      "Epoch 65/200 loss: 0.287 time: 63.10 sec lr: [0.009968437894841807]\n",
      "Epoch 66/200 loss: 0.278 time: 62.54 sec lr: [0.009954601214285258]\n",
      "Epoch 67/200 loss: 0.238 time: 63.40 sec lr: [0.009938269752880171]\n",
      "Epoch 68/200 loss: 0.179 time: 63.20 sec lr: [0.00991945173400918]\n",
      "Epoch 69/200 loss: 0.276 time: 62.63 sec lr: [0.009898156633111495]\n",
      "Epoch 70/200 loss: 0.238 time: 61.19 sec lr: [0.009874395172911719]\n",
      "Epoch 71/200 loss: 0.198 time: 61.23 sec lr: [0.009848179318020634]\n",
      "Epoch 72/200 loss: 0.178 time: 62.93 sec lr: [0.009819522268910676]\n",
      "Epoch 73/200 loss: 0.284 time: 63.52 sec lr: [0.00978843845526907]\n",
      "Epoch 74/200 loss: 0.165 time: 63.93 sec lr: [0.00975494352873208]\n",
      "Epoch 75/200 loss: 0.271 time: 63.35 sec lr: [0.009719054355003911]\n",
      "Epoch 76/200 loss: 0.212 time: 63.20 sec lr: [0.009680789005364343]\n",
      "Epoch 77/200 loss: 0.216 time: 63.02 sec lr: [0.009640166747569276]\n",
      "Epoch 78/200 loss: 0.187 time: 62.18 sec lr: [0.009597208036148849]\n",
      "Epoch 79/200 loss: 0.208 time: 63.23 sec lr: [0.009551934502107964]\n",
      "Epoch 80/200 loss: 0.169 time: 63.81 sec lr: [0.009504368942034424]\n",
      "Epoch 81/200 loss: 0.175 time: 63.79 sec lr: [0.009454535306620158]\n",
      "Epoch 82/200 loss: 0.213 time: 63.45 sec lr: [0.00940245868860134]\n",
      "Epoch 83/200 loss: 0.242 time: 64.03 sec lr: [0.009348165310123427]\n",
      "Epoch 84/200 loss: 0.190 time: 63.38 sec lr: [0.00929168250953753]\n",
      "Epoch 85/200 loss: 0.166 time: 63.38 sec lr: [0.009233038727634719]\n",
      "Epoch 86/200 loss: 0.174 time: 62.65 sec lr: [0.00917226349332524]\n",
      "Epoch 87/200 loss: 0.191 time: 63.22 sec lr: [0.009109387408769817]\n",
      "Epoch 88/200 loss: 0.172 time: 63.41 sec lr: [0.00904444213397053]\n",
      "Epoch 89/200 loss: 0.174 time: 63.44 sec lr: [0.00897746037082907]\n",
      "Epoch 90/200 loss: 0.142 time: 63.54 sec lr: [0.008908475846680334]\n",
      "Epoch 91/200 loss: 0.333 time: 63.00 sec lr: [0.008837523297309696]\n",
      "Epoch 92/200 loss: 0.286 time: 63.57 sec lr: [0.008764638449462504]\n",
      "Epoch 93/200 loss: 0.145 time: 60.30 sec lr: [0.00868985800285457]\n",
      "Epoch 94/200 loss: 0.184 time: 62.85 sec lr: [0.008613219611692778]\n",
      "Epoch 95/200 loss: 0.128 time: 63.23 sec lr: [0.00853476186571504]\n",
      "Epoch 96/200 loss: 0.129 time: 63.06 sec lr: [0.008454524270759208]\n",
      "Epoch 97/200 loss: 0.142 time: 63.00 sec lr: [0.008372547228870698]\n",
      "Epoch 98/200 loss: 0.165 time: 62.98 sec lr: [0.008288872017958838]\n",
      "Epoch 99/200 loss: 0.143 time: 62.75 sec lr: [0.008203540771012202]\n",
      "Epoch 100/200 loss: 0.131 time: 63.53 sec lr: [0.008116596454883376]\n",
      "Epoch 101/200 loss: 0.130 time: 61.94 sec lr: [0.008028082848653856]\n",
      "Epoch 102/200 loss: 0.161 time: 63.15 sec lr: [0.007938044521589966]\n",
      "Epoch 103/200 loss: 0.112 time: 62.53 sec lr: [0.007846526810700886]\n",
      "Epoch 104/200 loss: 0.112 time: 63.48 sec lr: [0.007753575797910103]\n",
      "Epoch 105/200 loss: 0.134 time: 63.72 sec lr: [0.007659238286851775]\n",
      "Epoch 106/200 loss: 0.154 time: 63.04 sec lr: [0.007563561779303695]\n",
      "Epoch 107/200 loss: 0.100 time: 62.28 sec lr: [0.007466594451268707]\n",
      "Epoch 108/200 loss: 0.139 time: 61.58 sec lr: [0.007368385128716649]\n",
      "Epoch 109/200 loss: 0.145 time: 63.92 sec lr: [0.007268983262998996]\n",
      "Epoch 110/200 loss: 0.108 time: 62.77 sec lr: [0.007168438905948624]\n",
      "Epoch 111/200 loss: 0.104 time: 63.55 sec lr: [0.007066802684677203]\n",
      "Epoch 112/200 loss: 0.158 time: 63.51 sec lr: [0.00696412577608291]\n",
      "Epoch 113/200 loss: 0.116 time: 63.15 sec lr: [0.006860459881081343]\n",
      "Epoch 114/200 loss: 0.102 time: 63.40 sec lr: [0.006755857198572525]\n",
      "Epoch 115/200 loss: 0.092 time: 63.74 sec lr: [0.006650370399157215]\n",
      "Epoch 116/200 loss: 0.112 time: 62.24 sec lr: [0.006544052598615647]\n",
      "Epoch 117/200 loss: 0.071 time: 63.45 sec lr: [0.006436957331162156]\n",
      "Epoch 118/200 loss: 0.115 time: 63.14 sec lr: [0.006329138522489074]\n",
      "Epoch 119/200 loss: 0.135 time: 63.65 sec lr: [0.0062206504626135355]\n",
      "Epoch 120/200 loss: 0.081 time: 62.98 sec lr: [0.006111547778540798]\n",
      "Epoch 121/200 loss: 0.087 time: 63.62 sec lr: [0.00600188540675792]\n",
      "Epoch 122/200 loss: 0.117 time: 63.89 sec lr: [0.005891718565571578]\n",
      "Epoch 123/200 loss: 0.076 time: 63.14 sec lr: [0.005781102727303978]\n",
      "Epoch 124/200 loss: 0.079 time: 61.67 sec lr: [0.005670093590360888]\n",
      "Epoch 125/200 loss: 0.087 time: 62.62 sec lr: [0.00555874705118579]\n",
      "Epoch 126/200 loss: 0.082 time: 63.89 sec lr: [0.005447119176114364]\n",
      "Epoch 127/200 loss: 0.068 time: 64.00 sec lr: [0.005335266173143378]\n",
      "Epoch 128/200 loss: 0.071 time: 62.91 sec lr: [0.005223244363628282]\n",
      "Epoch 129/200 loss: 0.064 time: 63.45 sec lr: [0.005111110153923697]\n",
      "Epoch 130/200 loss: 0.207 time: 62.52 sec lr: [0.004998920006981136]\n",
      "Epoch 131/200 loss: 0.061 time: 60.79 sec lr: [0.004886730413918185]\n",
      "Epoch 132/200 loss: 0.050 time: 63.80 sec lr: [0.004774597865573541]\n",
      "Epoch 133/200 loss: 0.054 time: 63.88 sec lr: [0.004662578824062164]\n",
      "Epoch 134/200 loss: 0.065 time: 63.89 sec lr: [0.00455072969434491]\n",
      "Epoch 135/200 loss: 0.059 time: 63.62 sec lr: [0.004439106795826925]\n",
      "Epoch 136/200 loss: 0.063 time: 62.86 sec lr: [0.004327766333999134]\n",
      "Epoch 137/200 loss: 0.067 time: 62.43 sec lr: [0.004216764372137084]\n",
      "Epoch 138/200 loss: 0.055 time: 60.97 sec lr: [0.004106156803071396]\n",
      "Epoch 139/200 loss: 0.052 time: 63.02 sec lr: [0.003995999321044042]\n",
      "Epoch 140/200 loss: 0.051 time: 63.25 sec lr: [0.0038863473936646114]\n",
      "Epoch 141/200 loss: 0.059 time: 63.48 sec lr: [0.0037772562339807047]\n",
      "Epoch 142/200 loss: 0.057 time: 64.01 sec lr: [0.0036687807726765048]\n",
      "Epoch 143/200 loss: 0.046 time: 62.79 sec lr: [0.003560975630413515]\n",
      "Epoch 144/200 loss: 0.092 time: 63.12 sec lr: [0.003453895090327413]\n",
      "Epoch 145/200 loss: 0.051 time: 61.17 sec lr: [0.0033475930706948626]\n",
      "Epoch 146/200 loss: 0.042 time: 62.83 sec lr: [0.003242123097784032]\n",
      "Epoch 147/200 loss: 0.038 time: 63.82 sec lr: [0.003137538278902509]\n",
      "Epoch 148/200 loss: 0.040 time: 63.09 sec lr: [0.003033891275656173]\n",
      "Epoch 149/200 loss: 0.082 time: 63.47 sec lr: [0.002931234277432484]\n",
      "Epoch 150/200 loss: 0.039 time: 63.55 sec lr: [0.0028296189751215606]\n",
      "Epoch 151/200 loss: 0.033 time: 64.12 sec lr: [0.0027290965350882652]\n",
      "Epoch 152/200 loss: 0.032 time: 61.41 sec lr: [0.0026297175734083817]\n",
      "Epoch 153/200 loss: 0.031 time: 63.56 sec lr: [0.002531532130381913]\n",
      "Epoch 154/200 loss: 0.037 time: 64.06 sec lr: [0.0024345896453362775]\n",
      "Epoch 155/200 loss: 0.048 time: 64.19 sec lr: [0.0023389389317321164]\n",
      "Epoch 156/200 loss: 0.055 time: 63.00 sec lr: [0.002244628152584249]\n",
      "Epoch 157/200 loss: 0.032 time: 63.52 sec lr: [0.002151704796210145]\n",
      "Epoch 158/200 loss: 0.030 time: 63.05 sec lr: [0.002060215652318124]\n",
      "Epoch 159/200 loss: 0.030 time: 63.83 sec lr: [0.0019702067884473286]\n",
      "Epoch 160/200 loss: 0.028 time: 61.69 sec lr: [0.0018817235267713378]\n",
      "Epoch 161/200 loss: 0.029 time: 63.30 sec lr: [0.001794810421277074]\n",
      "Epoch 162/200 loss: 0.030 time: 63.54 sec lr: [0.001709511235330541]\n",
      "Epoch 163/200 loss: 0.028 time: 63.15 sec lr: [0.0016258689196406461]\n",
      "Epoch 164/200 loss: 0.027 time: 63.92 sec lr: [0.0015439255906322304]\n",
      "Epoch 165/200 loss: 0.026 time: 63.53 sec lr: [0.0014637225092391675]\n",
      "Epoch 166/200 loss: 0.027 time: 62.86 sec lr: [0.0013853000601282554]\n",
      "Epoch 167/200 loss: 0.027 time: 61.50 sec lr: [0.0013086977313643117]\n",
      "Epoch 168/200 loss: 0.025 time: 63.83 sec lr: [0.0012339540945267484]\n",
      "Epoch 169/200 loss: 0.026 time: 62.65 sec lr: [0.0011611067852876154]\n",
      "Epoch 170/200 loss: 0.025 time: 63.78 sec lr: [0.001090192484460911]\n",
      "Epoch 171/200 loss: 0.029 time: 65.83 sec lr: [0.001021246899532685]\n",
      "Epoch 172/200 loss: 0.026 time: 66.25 sec lr: [0.0009543047466812432]\n",
      "Epoch 173/200 loss: 0.025 time: 63.51 sec lr: [0.000889399733296511]\n",
      "Epoch 174/200 loss: 0.025 time: 63.54 sec lr: [0.000826564541007338]\n",
      "Epoch 175/200 loss: 0.025 time: 61.43 sec lr: [0.0007658308092253238]\n",
      "Epoch 176/200 loss: 0.025 time: 64.12 sec lr: [0.0007072291192134158]\n",
      "Epoch 177/200 loss: 0.025 time: 63.43 sec lr: [0.000650788978687327]\n",
      "Epoch 178/200 loss: 0.024 time: 63.33 sec lr: [0.0005965388069575031]\n",
      "Epoch 179/200 loss: 0.024 time: 63.39 sec lr: [0.0005445059206191539]\n",
      "Epoch 180/200 loss: 0.024 time: 63.45 sec lr: [0.0004947165197975214]\n",
      "Epoch 181/200 loss: 0.024 time: 63.52 sec lr: [0.0004471956749553256]\n",
      "Epoch 182/200 loss: 0.024 time: 61.64 sec lr: [0.0004019673142690342]\n",
      "Epoch 183/200 loss: 0.024 time: 63.53 sec lr: [0.0003590542115803052]\n",
      "Epoch 184/200 loss: 0.023 time: 63.71 sec lr: [0.0003184779749286688]\n",
      "Epoch 185/200 loss: 0.023 time: 63.70 sec lr: [0.00028025903567123185]\n",
      "Epoch 186/200 loss: 0.024 time: 63.28 sec lr: [0.00024441663819487505]\n",
      "Epoch 187/200 loss: 0.024 time: 64.32 sec lr: [0.00021096883022611888]\n",
      "Epoch 188/200 loss: 0.023 time: 64.42 sec lr: [0.0001799324537435574]\n",
      "Epoch 189/200 loss: 0.023 time: 63.26 sec lr: [0.00015132313649740884]\n",
      "Epoch 190/200 loss: 0.023 time: 62.48 sec lr: [0.000125155284140476]\n",
      "Epoch 191/200 loss: 0.023 time: 63.58 sec lr: [0.00010144207297446546]\n",
      "Epoch 192/200 loss: 0.023 time: 62.88 sec lr: [8.01954433153219e-05]\n",
      "Epoch 193/200 loss: 0.023 time: 62.96 sec lr: [6.14260934809209e-05]\n",
      "Epoch 194/200 loss: 0.023 time: 63.07 sec lr: [4.514347440414295e-05]\n",
      "Epoch 195/200 loss: 0.023 time: 62.79 sec lr: [3.13557848740442e-05]\n",
      "Epoch 196/200 loss: 0.023 time: 62.66 sec lr: [2.006996740751865e-05]\n",
      "Epoch 197/200 loss: 0.023 time: 61.56 sec lr: [1.1291704753533407e-05]\n",
      "Epoch 198/200 loss: 0.024 time: 63.10 sec lr: [5.025417031691334e-06]\n",
      "Epoch 199/200 loss: 0.023 time: 62.36 sec lr: [1.2742595065702892e-06]\n",
      "Epoch 200/200 loss: 0.023 time: 63.56 sec lr: [4.01209989498081e-08]\n",
      "\n",
      "Model Accuracy: 98.19%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "        \n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "        else:\n",
    "            self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "        \n",
    "        self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "        lower_filename = filename.lower()\n",
    "        if \"adenosis\" in lower_filename:\n",
    "            label = 0\n",
    "        elif \"fibroadenoma\" in lower_filename:\n",
    "            label = 1\n",
    "        elif \"phyllodes_tumor\" in lower_filename:\n",
    "            label = 2\n",
    "        elif \"tubular_adenoma\" in lower_filename:\n",
    "            label = 3\n",
    "        elif \"ductal_carcinoma\" in lower_filename:\n",
    "            label = 4\n",
    "        elif \"lobular_carcinoma\" in lower_filename:\n",
    "            label = 5\n",
    "        elif \"mucinous_carcinoma\" in lower_filename:\n",
    "            label = 6\n",
    "        elif \"papillary_carcinoma\" in lower_filename:\n",
    "            label = 7\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                        kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((tokens_batch, x), dim=1)\n",
    "        x = x + self.pe\n",
    "        return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attention = Q @ K.transpose(-2, -1)\n",
    "        attention = attention / (self.head_size ** 0.5)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        out = attention @ V\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.W_o(out)\n",
    "        return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * r_mlp, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mha(self.ln1(x))\n",
    "        out = out + self.mlp(self.ln2(out))\n",
    "        return out\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "            \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_classes = n_classes\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.patch_embedding(images)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.RandomRotation(degrees=180),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "train_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=True, transform=test_transform)\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "optimizer = AdamW(transformer.parameters(), lr=alpha)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    transformer.train()\n",
    "    training_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        training_loss += loss.item()\n",
    "    end = time.time()\n",
    "    curr_lr = scheduler.get_last_lr()\n",
    "    print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss / len(train_loader):.3f} time: {end-start:.2f} sec lr: {curr_lr}')\n",
    "    if ((epoch+1) % 10) == 0:\n",
    "        model_scripted = torch.jit.script(transformer)\n",
    "        model_scripted.save(f\"./checkpoints/checkpoint:{epoch+1}-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}.pth\")\n",
    "\n",
    "# Testing\n",
    "\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = test_model(transformer, test_loader, device)\n",
    "\n",
    "# Save the final model\n",
    "model_scripted = torch.jit.script(transformer)\n",
    "model_scripted.save(f\"./models/nonbinary/{test_accuracy:.2f}%-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}.pth\")\n",
    "\n",
    "# model = torch.jit.load('model_scripted.pt')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (NVIDIA GeForce RTX 4050 Laptop GPU)\n",
      "\n",
      "Model Accuracy: 48.11%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "        \n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "        else:\n",
    "            self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "        \n",
    "        self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "        lower_filename = filename.lower()\n",
    "        if \"adenosis\" in lower_filename:\n",
    "            label = 0\n",
    "        elif \"fibroadenoma\" in lower_filename:\n",
    "            label = 1\n",
    "        elif \"phyllodes_tumor\" in lower_filename:\n",
    "            label = 2\n",
    "        elif \"tubular_adenoma\" in lower_filename:\n",
    "            label = 3\n",
    "        elif \"ductal_carcinoma\" in lower_filename:\n",
    "            label = 4\n",
    "        elif \"lobular_carcinoma\" in lower_filename:\n",
    "            label = 5\n",
    "        elif \"mucinous_carcinoma\" in lower_filename:\n",
    "            label = 6\n",
    "        elif \"papillary_carcinoma\" in lower_filename:\n",
    "            label = 7\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                        kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((tokens_batch, x), dim=1)\n",
    "        x = x + self.pe\n",
    "        return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attention = Q @ K.transpose(-2, -1)\n",
    "        attention = attention / (self.head_size ** 0.5)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        out = attention @ V\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.W_o(out)\n",
    "        return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * r_mlp, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mha(self.ln1(x))\n",
    "        out = out + self.mlp(self.ln2(out))\n",
    "        return out\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "            \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_classes = n_classes\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.patch_embedding(images)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.RandomRotation(degrees=180),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "model = torch.jit.load('./models/nonbinary/98.19%-16-8-(256, 256)-(16, 16)-3-4-8-256-200-0.001.pth')\n",
    "\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = test_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
