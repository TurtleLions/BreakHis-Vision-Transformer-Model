{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install \"numpy<2\"\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (NVIDIA GeForce RTX 4050 Laptop GPU)\n",
      "Epoch 1/200 loss: 1.657 time: 83.00 sec lr: [0.0004065803829630208]\n",
      "Epoch 2/200 loss: 1.526 time: 86.24 sec lr: [0.00042630348958544624]\n",
      "Epoch 3/200 loss: 1.462 time: 87.00 sec lr: [0.0004591152425361105]\n",
      "Epoch 4/200 loss: 1.416 time: 82.00 sec lr: [0.0005049256776899699]\n",
      "Epoch 5/200 loss: 1.369 time: 84.25 sec lr: [0.0005636091907941006]\n",
      "Epoch 6/200 loss: 1.319 time: 82.50 sec lr: [0.0006350048818527364]\n",
      "Epoch 7/200 loss: 1.282 time: 84.03 sec lr: [0.0007189169962870962]\n",
      "Epoch 8/200 loss: 1.230 time: 85.24 sec lr: [0.0008151154616604216]\n",
      "Epoch 9/200 loss: 1.204 time: 81.51 sec lr: [0.0009233365184966279]\n",
      "Epoch 10/200 loss: 1.169 time: 84.10 sec lr: [0.001043283443462946]\n",
      "Epoch 11/200 loss: 1.130 time: 83.79 sec lr: [0.0011746273629337396]\n",
      "Epoch 12/200 loss: 1.108 time: 84.22 sec lr: [0.0013170081547048546]\n",
      "Epoch 13/200 loss: 1.067 time: 82.22 sec lr: [0.0014700354353861089]\n",
      "Epoch 14/200 loss: 1.036 time: 83.03 sec lr: [0.0016332896307647247]\n",
      "Epoch 15/200 loss: 1.027 time: 82.00 sec lr: [0.001806323126204901]\n",
      "Epoch 16/200 loss: 0.990 time: 82.79 sec lr: [0.001988661493929381]\n",
      "Epoch 17/200 loss: 0.994 time: 82.43 sec lr: [0.0021798047938179615]\n",
      "Epoch 18/200 loss: 0.949 time: 83.33 sec lr: [0.0023792289441564446]\n",
      "Epoch 19/200 loss: 0.945 time: 81.34 sec lr: [0.002586387158577618]\n",
      "Epoch 20/200 loss: 0.911 time: 84.06 sec lr: [0.0028007114452544713]\n",
      "Epoch 21/200 loss: 0.900 time: 83.13 sec lr: [0.0030216141642350836]\n",
      "Epoch 22/200 loss: 0.883 time: 84.15 sec lr: [0.003248489638649262]\n",
      "Epoch 23/200 loss: 0.875 time: 82.08 sec lr: [0.0034807158153692718]\n",
      "Epoch 24/200 loss: 0.860 time: 82.83 sec lr: [0.003717655970571422]\n",
      "Epoch 25/200 loss: 0.851 time: 82.63 sec lr: [0.003958660455522151]\n",
      "Epoch 26/200 loss: 0.829 time: 82.96 sec lr: [0.004203068477801967]\n",
      "Epoch 27/200 loss: 0.815 time: 82.10 sec lr: [0.004450209913083438]\n",
      "Epoch 28/200 loss: 0.832 time: 83.78 sec lr: [0.004699407142495654]\n",
      "Epoch 29/200 loss: 0.808 time: 84.18 sec lr: [0.004949976910537425]\n",
      "Epoch 30/200 loss: 0.799 time: 84.21 sec lr: [0.005201232198445138]\n",
      "Epoch 31/200 loss: 0.769 time: 82.96 sec lr: [0.005452484107878828]\n",
      "Epoch 32/200 loss: 0.766 time: 82.65 sec lr: [0.00570304374976172]\n",
      "Epoch 33/200 loss: 0.763 time: 83.47 sec lr: [0.005952224133094366]\n",
      "Epoch 34/200 loss: 0.761 time: 81.24 sec lr: [0.0061993420485646]\n",
      "Epoch 35/200 loss: 0.733 time: 82.50 sec lr: [0.006443719941788748]\n",
      "Epoch 36/200 loss: 0.733 time: 81.22 sec lr: [0.006684687771048013]\n",
      "Epoch 37/200 loss: 0.745 time: 83.57 sec lr: [0.00692158484442644]\n",
      "Epoch 38/200 loss: 0.730 time: 82.82 sec lr: [0.0071537616313133425]\n",
      "Epoch 39/200 loss: 0.717 time: 83.74 sec lr: [0.007380581543303369]\n",
      "Epoch 40/200 loss: 0.718 time: 81.25 sec lr: [0.007601422679611298]\n",
      "Epoch 41/200 loss: 0.710 time: 80.54 sec lr: [0.007815679532215923]\n",
      "Epoch 42/200 loss: 0.692 time: 83.74 sec lr: [0.008022764646057829]\n",
      "Epoch 43/200 loss: 0.684 time: 81.03 sec lr: [0.008222110229739095]\n",
      "Epoch 44/200 loss: 0.690 time: 84.83 sec lr: [0.008413169712308632]\n",
      "Epoch 45/200 loss: 0.669 time: 82.02 sec lr: [0.00859541924186476]\n",
      "Epoch 46/200 loss: 0.688 time: 83.92 sec lr: [0.008768359121866106]\n",
      "Epoch 47/200 loss: 0.673 time: 81.72 sec lr: [0.008931515181212669]\n",
      "Epoch 48/200 loss: 0.674 time: 84.17 sec lr: [0.009084440074340604]\n",
      "Epoch 49/200 loss: 0.639 time: 81.65 sec lr: [0.009226714507766007]\n",
      "Epoch 50/200 loss: 0.673 time: 81.83 sec lr: [0.009357948389714803]\n",
      "Epoch 51/200 loss: 0.634 time: 82.96 sec lr: [0.009477781899686597]\n",
      "Epoch 52/200 loss: 0.641 time: 81.83 sec lr: [0.009585886475019964]\n",
      "Epoch 53/200 loss: 0.641 time: 83.03 sec lr: [0.009681965711754175]\n",
      "Epoch 54/200 loss: 0.625 time: 82.37 sec lr: [0.009765756177317356]\n",
      "Epoch 55/200 loss: 0.630 time: 81.89 sec lr: [0.009837028132812814]\n",
      "Epoch 56/200 loss: 0.635 time: 80.83 sec lr: [0.009895586162923182]\n",
      "Epoch 57/200 loss: 0.644 time: 82.96 sec lr: [0.009941269711705255]\n",
      "Epoch 58/200 loss: 0.612 time: 81.60 sec lr: [0.009973953522806495]\n",
      "Epoch 59/200 loss: 0.620 time: 84.68 sec lr: [0.00999354798289618]\n",
      "Epoch 60/200 loss: 0.603 time: 82.73 sec lr: [0.00999999987900105]\n",
      "Epoch 61/200 loss: 0.613 time: 84.89 sec lr: [0.009998716377065266]\n",
      "Epoch 62/200 loss: 0.626 time: 84.12 sec lr: [0.009994915880967953]\n",
      "Epoch 63/200 loss: 0.598 time: 85.90 sec lr: [0.00998860030437335]\n",
      "Epoch 64/200 loss: 0.593 time: 83.89 sec lr: [0.009979772827364463]\n",
      "Epoch 65/200 loss: 0.570 time: 83.11 sec lr: [0.009968437894841807]\n",
      "Epoch 66/200 loss: 0.601 time: 81.93 sec lr: [0.009954601214285258]\n",
      "Epoch 67/200 loss: 0.568 time: 81.62 sec lr: [0.009938269752880171]\n",
      "Epoch 68/200 loss: 0.572 time: 81.31 sec lr: [0.00991945173400918]\n",
      "Epoch 69/200 loss: 0.569 time: 83.31 sec lr: [0.009898156633111495]\n",
      "Epoch 70/200 loss: 0.584 time: 81.39 sec lr: [0.009874395172911719]\n",
      "Epoch 71/200 loss: 0.572 time: 83.57 sec lr: [0.009848179318020634]\n",
      "Epoch 72/200 loss: 0.560 time: 81.97 sec lr: [0.009819522268910676]\n",
      "Epoch 73/200 loss: 0.551 time: 84.11 sec lr: [0.00978843845526907]\n",
      "Epoch 74/200 loss: 0.538 time: 82.16 sec lr: [0.00975494352873208]\n",
      "Epoch 75/200 loss: 0.538 time: 81.24 sec lr: [0.009719054355003911]\n",
      "Epoch 76/200 loss: 0.531 time: 83.16 sec lr: [0.009680789005364343]\n",
      "Epoch 77/200 loss: 0.539 time: 81.06 sec lr: [0.009640166747569276]\n",
      "Epoch 78/200 loss: 0.527 time: 83.91 sec lr: [0.009597208036148849]\n",
      "Epoch 79/200 loss: 0.531 time: 81.26 sec lr: [0.009551934502107964]\n",
      "Epoch 80/200 loss: 0.530 time: 83.11 sec lr: [0.009504368942034424]\n",
      "Epoch 81/200 loss: 0.525 time: 81.13 sec lr: [0.009454535306620158]\n",
      "Epoch 82/200 loss: 0.513 time: 80.96 sec lr: [0.00940245868860134]\n",
      "Epoch 83/200 loss: 0.512 time: 83.49 sec lr: [0.009348165310123427]\n",
      "Epoch 84/200 loss: 0.500 time: 81.78 sec lr: [0.00929168250953753]\n",
      "Epoch 85/200 loss: 0.509 time: 83.28 sec lr: [0.009233038727634719]\n",
      "Epoch 86/200 loss: 0.507 time: 81.45 sec lr: [0.00917226349332524]\n",
      "Epoch 87/200 loss: 0.497 time: 81.72 sec lr: [0.009109387408769817]\n",
      "Epoch 88/200 loss: 0.495 time: 84.50 sec lr: [0.00904444213397053]\n",
      "Epoch 89/200 loss: 0.493 time: 82.11 sec lr: [0.00897746037082907]\n",
      "Epoch 90/200 loss: 0.492 time: 84.25 sec lr: [0.008908475846680334]\n",
      "Epoch 91/200 loss: 0.505 time: 81.38 sec lr: [0.008837523297309696]\n",
      "Epoch 92/200 loss: 0.485 time: 83.97 sec lr: [0.008764638449462504]\n",
      "Epoch 93/200 loss: 0.479 time: 81.38 sec lr: [0.00868985800285457]\n",
      "Epoch 94/200 loss: 0.474 time: 81.09 sec lr: [0.008613219611692778]\n",
      "Epoch 95/200 loss: 0.470 time: 83.90 sec lr: [0.00853476186571504]\n",
      "Epoch 96/200 loss: 0.489 time: 83.79 sec lr: [0.008454524270759208]\n",
      "Epoch 97/200 loss: 0.458 time: 83.60 sec lr: [0.008372547228870698]\n",
      "Epoch 98/200 loss: 0.469 time: 82.76 sec lr: [0.008288872017958838]\n",
      "Epoch 99/200 loss: 0.449 time: 82.20 sec lr: [0.008203540771012202]\n",
      "Epoch 100/200 loss: 0.461 time: 81.11 sec lr: [0.008116596454883376]\n",
      "Epoch 101/200 loss: 0.455 time: 84.86 sec lr: [0.008028082848653856]\n",
      "Epoch 102/200 loss: 0.458 time: 80.52 sec lr: [0.007938044521589966]\n",
      "Epoch 103/200 loss: 0.431 time: 82.13 sec lr: [0.007846526810700886]\n",
      "Epoch 104/200 loss: 0.438 time: 82.97 sec lr: [0.007753575797910103]\n",
      "Epoch 105/200 loss: 0.442 time: 81.23 sec lr: [0.007659238286851775]\n",
      "Epoch 106/200 loss: 0.449 time: 83.33 sec lr: [0.007563561779303695]\n",
      "Epoch 107/200 loss: 0.419 time: 82.64 sec lr: [0.007466594451268707]\n",
      "Epoch 108/200 loss: 0.431 time: 84.15 sec lr: [0.007368385128716649]\n",
      "Epoch 109/200 loss: 0.432 time: 81.68 sec lr: [0.007268983262998996]\n",
      "Epoch 110/200 loss: 0.445 time: 81.18 sec lr: [0.007168438905948624]\n",
      "Epoch 111/200 loss: 0.418 time: 82.72 sec lr: [0.007066802684677203]\n",
      "Epoch 112/200 loss: 0.424 time: 81.79 sec lr: [0.00696412577608291]\n",
      "Epoch 113/200 loss: 0.405 time: 83.64 sec lr: [0.006860459881081343]\n",
      "Epoch 114/200 loss: 0.413 time: 80.35 sec lr: [0.006755857198572525]\n",
      "Epoch 115/200 loss: 0.409 time: 81.06 sec lr: [0.006650370399157215]\n",
      "Epoch 116/200 loss: 0.397 time: 83.55 sec lr: [0.006544052598615647]\n",
      "Epoch 117/200 loss: 0.399 time: 82.25 sec lr: [0.006436957331162156]\n",
      "Epoch 118/200 loss: 0.408 time: 83.73 sec lr: [0.006329138522489074]\n",
      "Epoch 119/200 loss: 0.395 time: 81.06 sec lr: [0.0062206504626135355]\n",
      "Epoch 120/200 loss: 0.395 time: 81.20 sec lr: [0.006111547778540798]\n",
      "Epoch 121/200 loss: 0.388 time: 84.17 sec lr: [0.00600188540675792]\n",
      "Epoch 122/200 loss: 0.388 time: 81.65 sec lr: [0.005891718565571578]\n",
      "Epoch 123/200 loss: 0.371 time: 82.42 sec lr: [0.005781102727303978]\n",
      "Epoch 124/200 loss: 0.382 time: 81.15 sec lr: [0.005670093590360888]\n",
      "Epoch 125/200 loss: 0.376 time: 83.95 sec lr: [0.00555874705118579]\n",
      "Epoch 126/200 loss: 0.375 time: 82.31 sec lr: [0.005447119176114364]\n",
      "Epoch 127/200 loss: 0.366 time: 81.30 sec lr: [0.005335266173143378]\n",
      "Epoch 128/200 loss: 0.379 time: 83.97 sec lr: [0.005223244363628282]\n",
      "Epoch 129/200 loss: 0.387 time: 80.91 sec lr: [0.005111110153923697]\n",
      "Epoch 130/200 loss: 0.374 time: 84.30 sec lr: [0.004998920006981136]\n",
      "Epoch 131/200 loss: 0.354 time: 81.56 sec lr: [0.004886730413918185]\n",
      "Epoch 132/200 loss: 0.359 time: 81.53 sec lr: [0.004774597865573541]\n",
      "Epoch 133/200 loss: 0.356 time: 82.97 sec lr: [0.004662578824062164]\n",
      "Epoch 134/200 loss: 0.358 time: 80.79 sec lr: [0.00455072969434491]\n",
      "Epoch 135/200 loss: 0.357 time: 82.98 sec lr: [0.004439106795826925]\n",
      "Epoch 136/200 loss: 0.345 time: 81.16 sec lr: [0.004327766333999134]\n",
      "Epoch 137/200 loss: 0.334 time: 81.26 sec lr: [0.004216764372137084]\n",
      "Epoch 138/200 loss: 0.343 time: 82.92 sec lr: [0.004106156803071396]\n",
      "Epoch 139/200 loss: 0.335 time: 81.22 sec lr: [0.003995999321044042]\n",
      "Epoch 140/200 loss: 0.332 time: 83.34 sec lr: [0.0038863473936646114]\n",
      "Epoch 141/200 loss: 0.330 time: 81.81 sec lr: [0.0037772562339807047]\n",
      "Epoch 142/200 loss: 0.339 time: 83.00 sec lr: [0.0036687807726765048]\n",
      "Epoch 143/200 loss: 0.331 time: 81.16 sec lr: [0.003560975630413515]\n",
      "Epoch 144/200 loss: 0.323 time: 81.33 sec lr: [0.003453895090327413]\n",
      "Epoch 145/200 loss: 0.319 time: 83.94 sec lr: [0.0033475930706948626]\n",
      "Epoch 146/200 loss: 0.327 time: 80.37 sec lr: [0.003242123097784032]\n",
      "Epoch 147/200 loss: 0.304 time: 83.58 sec lr: [0.003137538278902509]\n",
      "Epoch 148/200 loss: 0.321 time: 80.77 sec lr: [0.003033891275656173]\n",
      "Epoch 149/200 loss: 0.298 time: 81.98 sec lr: [0.002931234277432484]\n",
      "Epoch 150/200 loss: 0.305 time: 83.31 sec lr: [0.0028296189751215606]\n",
      "Epoch 151/200 loss: 0.312 time: 81.11 sec lr: [0.0027290965350882652]\n",
      "Epoch 152/200 loss: 0.302 time: 82.54 sec lr: [0.0026297175734083817]\n",
      "Epoch 153/200 loss: 0.304 time: 80.06 sec lr: [0.002531532130381913]\n",
      "Epoch 154/200 loss: 0.306 time: 84.42 sec lr: [0.0024345896453362775]\n",
      "Epoch 155/200 loss: 0.287 time: 80.86 sec lr: [0.0023389389317321164]\n",
      "Epoch 156/200 loss: 0.287 time: 82.57 sec lr: [0.002244628152584249]\n",
      "Epoch 157/200 loss: 0.294 time: 80.51 sec lr: [0.002151704796210145]\n",
      "Epoch 158/200 loss: 0.280 time: 81.64 sec lr: [0.002060215652318124]\n",
      "Epoch 159/200 loss: 0.288 time: 83.24 sec lr: [0.0019702067884473286]\n",
      "Epoch 160/200 loss: 0.281 time: 81.45 sec lr: [0.0018817235267713378]\n",
      "Epoch 161/200 loss: 0.275 time: 83.04 sec lr: [0.001794810421277074]\n",
      "Epoch 162/200 loss: 0.277 time: 81.29 sec lr: [0.001709511235330541]\n",
      "Epoch 163/200 loss: 0.275 time: 83.29 sec lr: [0.0016258689196406461]\n",
      "Epoch 164/200 loss: 0.282 time: 81.42 sec lr: [0.0015439255906322304]\n",
      "Epoch 165/200 loss: 0.269 time: 80.77 sec lr: [0.0014637225092391675]\n",
      "Epoch 166/200 loss: 0.264 time: 82.33 sec lr: [0.0013853000601282554]\n",
      "Epoch 167/200 loss: 0.270 time: 81.13 sec lr: [0.0013086977313643117]\n",
      "Epoch 168/200 loss: 0.269 time: 83.07 sec lr: [0.0012339540945267484]\n",
      "Epoch 169/200 loss: 0.261 time: 81.26 sec lr: [0.0011611067852876154]\n",
      "Epoch 170/200 loss: 0.265 time: 80.39 sec lr: [0.001090192484460911]\n",
      "Epoch 171/200 loss: 0.258 time: 83.25 sec lr: [0.001021246899532685]\n",
      "Epoch 172/200 loss: 0.258 time: 80.96 sec lr: [0.0009543047466812432]\n",
      "Epoch 173/200 loss: 0.259 time: 83.54 sec lr: [0.000889399733296511]\n",
      "Epoch 174/200 loss: 0.252 time: 80.86 sec lr: [0.000826564541007338]\n",
      "Epoch 175/200 loss: 0.255 time: 80.44 sec lr: [0.0007658308092253238]\n",
      "Epoch 176/200 loss: 0.249 time: 83.19 sec lr: [0.0007072291192134158]\n",
      "Epoch 177/200 loss: 0.248 time: 80.50 sec lr: [0.000650788978687327]\n",
      "Epoch 178/200 loss: 0.246 time: 81.48 sec lr: [0.0005965388069575031]\n",
      "Epoch 179/200 loss: 0.244 time: 83.64 sec lr: [0.0005445059206191539]\n",
      "Epoch 180/200 loss: 0.245 time: 81.43 sec lr: [0.0004947165197975214]\n",
      "Epoch 181/200 loss: 0.249 time: 85.49 sec lr: [0.0004471956749553256]\n",
      "Epoch 182/200 loss: 0.241 time: 84.19 sec lr: [0.0004019673142690342]\n",
      "Epoch 183/200 loss: 0.246 time: 86.48 sec lr: [0.0003590542115803052]\n",
      "Epoch 184/200 loss: 0.239 time: 82.96 sec lr: [0.0003184779749286688]\n",
      "Epoch 185/200 loss: 0.242 time: 85.27 sec lr: [0.00028025903567123185]\n",
      "Epoch 186/200 loss: 0.239 time: 82.28 sec lr: [0.00024441663819487505]\n",
      "Epoch 187/200 loss: 0.239 time: 83.00 sec lr: [0.00021096883022611888]\n",
      "Epoch 188/200 loss: 0.240 time: 81.08 sec lr: [0.0001799324537435574]\n",
      "Epoch 189/200 loss: 0.230 time: 81.49 sec lr: [0.00015132313649740884]\n",
      "Epoch 190/200 loss: 0.235 time: 83.53 sec lr: [0.000125155284140476]\n",
      "Epoch 191/200 loss: 0.231 time: 81.53 sec lr: [0.00010144207297446546]\n",
      "Epoch 192/200 loss: 0.236 time: 83.43 sec lr: [8.01954433153219e-05]\n",
      "Epoch 193/200 loss: 0.238 time: 81.10 sec lr: [6.14260934809209e-05]\n",
      "Epoch 194/200 loss: 0.236 time: 81.72 sec lr: [4.514347440414295e-05]\n",
      "Epoch 195/200 loss: 0.233 time: 84.47 sec lr: [3.13557848740442e-05]\n",
      "Epoch 196/200 loss: 0.233 time: 84.90 sec lr: [2.006996740751865e-05]\n",
      "Epoch 197/200 loss: 0.236 time: 85.38 sec lr: [1.1291704753533407e-05]\n",
      "Epoch 198/200 loss: 0.233 time: 81.80 sec lr: [5.025417031691334e-06]\n",
      "Epoch 199/200 loss: 0.239 time: 84.58 sec lr: [1.2742595065702892e-06]\n",
      "Epoch 200/200 loss: 0.231 time: 81.47 sec lr: [4.01209989498081e-08]\n",
      "\n",
      "Model Accuracy: 92.72%\n"
     ]
    }
   ],
   "source": [
    "# TRAINING AND TESTING\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "  def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "    \n",
    "    self.data_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    if train:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "    else:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "    \n",
    "    self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data_frame.iloc[idx]\n",
    "    filename = row['filename']\n",
    "    \n",
    "    img_path = os.path.join(self.root_dir, filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "  \n",
    "    lower_filename = filename.lower()\n",
    "    if \"adenosis\" in lower_filename:\n",
    "      label = 0\n",
    "    elif \"fibroadenoma\" in lower_filename:\n",
    "      label = 1\n",
    "    elif \"phyllodes_tumor\" in lower_filename:\n",
    "      label = 2\n",
    "    elif \"tubular_adenoma\" in lower_filename:\n",
    "      label = 3\n",
    "    elif \"ductal_carcinoma\" in lower_filename:\n",
    "      label = 4\n",
    "    elif \"lobular_carcinoma\" in lower_filename:\n",
    "      label = 5\n",
    "    elif \"mucinous_carcinoma\" in lower_filename:\n",
    "      label = 6\n",
    "    elif \"papillary_carcinoma\" in lower_filename:\n",
    "      label = 7\n",
    "    else:\n",
    "      raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1, 2)\n",
    "    return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "    x = torch.cat((tokens_batch, x), dim=1)\n",
    "    x = x + self.pe\n",
    "    return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "    attention = Q @ K.transpose(-2, -1)\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "    out = attention @ V\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.W_o(out)\n",
    "    return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.dropout1 = nn.Dropout(dropout_prob)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        nn.Linear(d_model * r_mlp, d_model),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    attn_out = self.mha(self.ln1(x))\n",
    "    x = x + self.dropout1(attn_out)\n",
    "    mlp_out = self.mlp(self.ln2(x))\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "        \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_classes = n_classes\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(\n",
    "        *[TransformerEncoder(self.d_model, self.n_heads, r_mlp=4, dropout_prob=dropout_prob)\n",
    "          for _ in range(n_layers)]\n",
    "    )\n",
    "    self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[:, 0])\n",
    "    return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "dropout_prob = 0.1\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "train_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=True, transform=train_transform)\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "  print(\"Using device:\", device)\n",
    "\n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, dropout_prob).to(device)\n",
    "optimizer = AdamW(transformer.parameters(), lr=alpha, weight_decay=1e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "  transformer.train()\n",
    "  training_loss = 0.0\n",
    "  for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    training_loss += loss.item()\n",
    "  end = time.time()\n",
    "  curr_lr = scheduler.get_last_lr()\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss / len(train_loader):.3f} time: {end-start:.2f} sec lr: {curr_lr}')\n",
    "  if ((epoch+1) % 10) == 0:\n",
    "    model_scripted = torch.jit.script(transformer)\n",
    "    model_scripted.save(f\"./checkpoints/checkpoint:{epoch+1}-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}-{dropout_prob}.pth\")\n",
    "\n",
    "# Testing\n",
    "\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy\n",
    "\n",
    "test_accuracy = test_model(transformer, test_loader, device)\n",
    "\n",
    "# Save the final model\n",
    "model_scripted = torch.jit.script(transformer)\n",
    "model_scripted.save(f\"./models/nonbinary/{test_accuracy:.2f}%-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}-{dropout_prob}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (NVIDIA GeForce RTX 4050 Laptop GPU)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 249\u001b[0m\n\u001b[1;32m    246\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m--> 249\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 243\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m    241\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    242\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 243\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FOR TESTING ONLY\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# dataset\n",
    "class BreakHisDataset(Dataset):\n",
    "  def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "    \n",
    "    self.data_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    if train:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "    else:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "    \n",
    "    self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data_frame.iloc[idx]\n",
    "    filename = row['filename']\n",
    "    \n",
    "    img_path = os.path.join(self.root_dir, filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "  \n",
    "    lower_filename = filename.lower()\n",
    "    if \"adenosis\" in lower_filename:\n",
    "      label = 0\n",
    "    elif \"fibroadenoma\" in lower_filename:\n",
    "      label = 1\n",
    "    elif \"phyllodes_tumor\" in lower_filename:\n",
    "      label = 2\n",
    "    elif \"tubular_adenoma\" in lower_filename:\n",
    "      label = 3\n",
    "    elif \"ductal_carcinoma\" in lower_filename:\n",
    "      label = 4\n",
    "    elif \"lobular_carcinoma\" in lower_filename:\n",
    "      label = 5\n",
    "    elif \"mucinous_carcinoma\" in lower_filename:\n",
    "      label = 6\n",
    "    elif \"papillary_carcinoma\" in lower_filename:\n",
    "      label = 7\n",
    "    else:\n",
    "      raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label\n",
    "\n",
    "# ViT components\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1, 2)\n",
    "    return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "    x = torch.cat((tokens_batch, x), dim=1)\n",
    "    x = x + self.pe\n",
    "    return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.dropout1 = nn.Dropout(dropout_prob)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        nn.Linear(d_model * r_mlp, d_model),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    attn_out = self.mha(self.ln1(x))\n",
    "    x = x + self.dropout1(attn_out)\n",
    "    mlp_out = self.mlp(self.ln2(x))\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.W_o(out)\n",
    "    return out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model * r_mlp),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(d_model * r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "    return out\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "        \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_classes = n_classes\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(\n",
    "        *[TransformerEncoder(self.d_model, self.n_heads, r_mlp=4, dropout_prob=dropout_prob)\n",
    "          for _ in range(n_layers)]\n",
    "    )\n",
    "    self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[:, 0])\n",
    "    return x\n",
    "\n",
    "# hyperparameters\n",
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "dropout_prob = 0.1\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "# CHANGE WHEN LOADING A MODEL\n",
    "model = torch.jit.load('./models/nonbinary/92.72%-16-8-(256, 256)-(16, 16)-3-4-8-256-200-0.001-0.1.pth')\n",
    "\n",
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "  print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy\n",
    "\n",
    "test_accuracy = test_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
